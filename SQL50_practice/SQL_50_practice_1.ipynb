{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a54c89-64e2-4911-a0c5-8446e8c9f4a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL50 - Problem1**\n",
    "\n",
    "Link: https://leetcode.com/problems/recyclable-and-low-fat-products/description/?envType=study-plan-v2&envId=top-sql-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acfb52af-064b-428c-a30a-07b6fc9213d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ProductData\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"low_fats\", StringType(), True),\n",
    "    StructField(\"recyclable\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create data\n",
    "data = [\n",
    "    (0, \"Y\", \"N\"),\n",
    "    (1, \"Y\", \"Y\"),\n",
    "    (2, \"N\", \"Y\"),\n",
    "    (3, \"Y\", \"Y\"),\n",
    "    (4, \"N\", \"N\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "products_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show input\n",
    "display(products_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75545eb7-597a-4b7d-904b-9882643d8952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SOLUTION-1** - We can write this solution like **pandas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "307e03c2-b7fa-441f-b822-c27a7ce6e44c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "products_df = products_df[(products_df[\"low_fats\"] == 'Y') & (products_df[\"recyclable\"] == 'Y')]\n",
    "display(products_df[['product_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d715383a-0ccc-4d93-889c-5c32907327de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SOLUTION-2** - In Pyspark, we have something called **_.filter_** to filter according to the conditions\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ `.filter(...)`\n",
    "\n",
    "`.filter()` is used to **keep only the rows** in your DataFrame that meet a certain **condition**, similar to a **WHERE clause** in SQL.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "df.filter(df.age > 30)\n",
    "```\n",
    "\n",
    "This returns only the rows where the `age` column is greater than 30.\n",
    "\n",
    "You can also combine conditions:\n",
    "\n",
    "```python\n",
    "df.filter((df.age > 30) & (df.gender == \"M\"))\n",
    "```\n",
    "\n",
    "This keeps rows where `age > 30` **and** `gender == \"M\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ `.select(...)`\n",
    "\n",
    "`.select()` is used to **choose specific columns** from the DataFrame, just like the `SELECT column_name` in SQL.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "df.select(\"name\", \"age\")\n",
    "```\n",
    "\n",
    "This gives you only the `name` and `age` columns, dropping the rest.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ù Combined Example (like your case):\n",
    "\n",
    "```python\n",
    "products_df.filter(\n",
    "    (products_df.low_fats == \"Y\") & (products_df.recyclable == \"Y\")\n",
    ").select(\"product_id\").show()\n",
    "```\n",
    "\n",
    "* `.filter(...)` ‚Üí keeps rows where both conditions are true.\n",
    "* `.select(\"product_id\")` ‚Üí keeps only the `product_id` column from those rows.\n",
    "* `.show()` ‚Üí displays the result in a nice table format.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f26c912d-fbf8-4552-8f27-c58c2d91f64c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Solution using PySpark code\n",
    "\n",
    "product_pdf = products_df.filter(\n",
    "    (products_df.low_fats == 'Y') & (products_df.recyclable == 'Y')\n",
    ").select('product_id').show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SQL_50_practice_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
